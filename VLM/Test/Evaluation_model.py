import json
import sys
from pathlib import Path
from typing import Dict, List
import re
from difflib import SequenceMatcher
from datetime import datetime

# ============================================
# è¯„ä¼°å™¨
# ============================================

class SpreadsheetEvaluator:
    """ç”µå­è¡¨æ ¼ç†è§£ä»»åŠ¡è¯„ä¼°å™¨"""
    
    def __init__(self, qa_file: str):
        with open(qa_file, 'r', encoding='utf-8') as f:
            self.qa_data = json.load(f)
        self.qa_dict = {qa['id']: qa for qa in self.qa_data['qa_pairs']}
    
    def normalize_text(self, text: str) -> str:
        """æ–‡æœ¬æ ‡å‡†åŒ–"""
        text = text.lower().strip()
        text = re.sub(r'[^\w\s:,]', '', text)
        text = re.sub(r'\s+', ' ', text)
        return text
    
    def parse_cell_range(self, text: str) -> Dict:
        """è§£æå•å…ƒæ ¼èŒƒå›´"""
        text_upper = text.upper()
        result = {'has_range': False, 'top_left': None, 'bottom_right': None}
        
        # Pattern: "A19:F54"
        pattern = r'([A-Z]+\d+)\s*:\s*([A-Z]+\d+)'
        match = re.search(pattern, text_upper)
        
        if match:
            result['has_range'] = True
            result['top_left'] = match.group(1)
            result['bottom_right'] = match.group(2)
        
        return result
    
    def evaluate_range(self, predicted: str, answer: str) -> float:
        """è¯„ä¼°rangeç±»å‹ç­”æ¡ˆ"""
        pred_range = self.parse_cell_range(predicted)
        ans_range = self.parse_cell_range(answer)
        
        if not pred_range['has_range']:
            return 0.0
        
        # ç²¾ç¡®åŒ¹é…
        if (pred_range['top_left'] == ans_range['top_left'] and 
            pred_range['bottom_right'] == ans_range['bottom_right']):
            return 1.0
        
        # éƒ¨åˆ†æ­£ç¡®
        if pred_range['top_left'] == ans_range['top_left']:
            return 0.5
        if pred_range['bottom_right'] == ans_range['bottom_right']:
            return 0.3
        
        return 0.0
    
    def evaluate_list(self, predicted: str, answer: List[str]) -> float:
        """è¯„ä¼°listç±»å‹ç­”æ¡ˆ"""
        pred_lower = predicted.lower()
        correct = sum(1 for item in answer if item.lower() in pred_lower)
        return correct / len(answer)
    
    def evaluate_single_value(self, predicted: str, answer: str, alternatives: List[str] = None) -> float:
        """è¯„ä¼°å•å€¼ç­”æ¡ˆ"""
        pred_norm = self.normalize_text(predicted)
        ans_norm = self.normalize_text(answer)
        
        # ç²¾ç¡®åŒ¹é…
        if ans_norm in pred_norm or pred_norm in ans_norm:
            return 1.0
        
        # æ£€æŸ¥å¤‡é€‰ç­”æ¡ˆ
        if alternatives:
            for alt in alternatives:
                if self.normalize_text(alt) in pred_norm:
                    return 1.0
        
        # æ¨¡ç³ŠåŒ¹é…
        similarity = SequenceMatcher(None, pred_norm, ans_norm).ratio()
        return similarity if similarity >= 0.8 else 0.0
    
    def evaluate_keywords(self, predicted: str, keywords: List[str], min_keywords: int = 1) -> float:
        """è¯„ä¼°å…³é”®è¯åŒ¹é…"""
        pred_lower = predicted.lower()
        matched = sum(1 for kw in keywords if kw.lower() in pred_lower)
        return matched / len(keywords) if matched >= min_keywords else 0.0
    
    def evaluate_single_qa(self, qa_id: str, vlm_answer: str) -> Dict:
        """è¯„ä¼°å•ä¸ªQAå¯¹"""
        qa_item = self.qa_dict.get(qa_id)
        if not qa_item:
            return {'score': 0.0, 'error': 'QA not found'}
        
        # æ£€æŸ¥é”™è¯¯
        if vlm_answer.startswith("ERROR"):
            return {
                'qa_id': qa_id,
                'category': qa_item['category'],
                'score': 0.0,
                'error': True,
                'expected': qa_item['answer'],
                'predicted': vlm_answer
            }
        
        answer_type = qa_item.get('answer_type', 'single_value')
        score = 0.0
        
        # æ ¹æ®ç±»å‹è¯„ä¼°
        if answer_type in ['range', 'cell_range']:
            score = self.evaluate_range(vlm_answer, qa_item['answer'])
        elif answer_type == 'list':
            score = self.evaluate_list(vlm_answer, qa_item['answer'])
        elif answer_type in ['single_value', 'numeric', 'cell_address']:
            score = self.evaluate_single_value(
                vlm_answer, 
                qa_item['answer'],
                qa_item.get('alternative_answers')
            )
        elif answer_type == 'semantic_reasoning':
            score = self.evaluate_keywords(
                vlm_answer,
                qa_item.get('answer_keywords', []),
                qa_item.get('min_keywords', 1)
            )
        
        return {
            'qa_id': qa_id,
            'category': qa_item['category'],
            'difficulty': qa_item.get('difficulty', 'medium'),
            'score': score,
            'expected': qa_item['answer'],
            'predicted': vlm_answer[:100]  # æˆªæ–­æ˜¾ç¤º
        }
    
    def evaluate_all(self, responses: Dict[str, str]) -> Dict:
        """è¯„ä¼°æ‰€æœ‰å›ç­”"""
        results = []
        
        for qa_id, answer in responses.items():
            result = self.evaluate_single_qa(qa_id, answer)
            results.append(result)
        
        return self.generate_summary(results)
    
    def generate_summary(self, results: List[Dict]) -> Dict:
        """ç”Ÿæˆè¯„ä¼°æ‘˜è¦"""
        total = len(results)
        total_score = sum(r['score'] for r in results)
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        by_category = {}
        for r in results:
            cat = r['category']
            if cat not in by_category:
                by_category[cat] = []
            by_category[cat].append(r)
        
        category_scores = {
            cat: sum(r['score'] for r in rs) / len(rs)
            for cat, rs in by_category.items()
        }
        
        # æŒ‰éš¾åº¦ç»Ÿè®¡
        by_difficulty = {}
        for r in results:
            diff = r.get('difficulty', 'medium')
            if diff not in by_difficulty:
                by_difficulty[diff] = []
            by_difficulty[diff].append(r)
        
        difficulty_scores = {
            diff: sum(r['score'] for r in rs) / len(rs)
            for diff, rs in by_difficulty.items()
        }
        
        # é”™è¯¯ç»Ÿè®¡
        errors = [r for r in results if r.get('error', False)]
        low_scores = [r for r in results if r['score'] < 0.5 and not r.get('error')]
        
        return {
            'overall': {
                'total_questions': total,
                'average_score': total_score / total if total > 0 else 0,
                'percentage': (total_score / total) * 100 if total > 0 else 0
            },
            'by_category': category_scores,
            'by_difficulty': difficulty_scores,
            'errors': errors,
            'low_scores': low_scores,
            'detailed_results': results
        }

# ============================================
# ä¸»è¯„ä¼°å‡½æ•°
# ============================================

def evaluate_model(model_name: str, evaluator: SpreadsheetEvaluator, save_report: bool = True):
    """è¯„ä¼°æŒ‡å®šæ¨¡å‹çš„ç»“æœ"""
    
    model_dir = Path("outputs") / model_name
    
    # æŸ¥æ‰¾æœ€æ–°çš„å“åº”æ–‡ä»¶
    response_files = list(model_dir.glob("responses_*.json"))
    if not response_files:
        print(f"âŒ No response files found for {model_name}")
        return None
    
    latest_response = max(response_files, key=lambda p: p.stat().st_mtime)
    
    # åŠ è½½å“åº”
    with open(latest_response, 'r', encoding='utf-8') as f:
        responses = json.load(f)
    
    # è¯„ä¼°
    print(f"\n{'='*70}")
    print(f"ğŸ“Š Evaluating: {model_name}")
    print(f"{'='*70}")
    print(f"ğŸ“‚ File: {latest_response.name}")
    
    summary = evaluator.evaluate_all(responses)
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nğŸ¯ Overall Performance:")
    print(f"   Total Questions: {summary['overall']['total_questions']}")
    print(f"   Average Score: {summary['overall']['average_score']:.3f}")
    print(f"   Percentage: {summary['overall']['percentage']:.1f}%")
    
    print(f"\nğŸ“‘ By Category:")
    for cat, score in sorted(summary['by_category'].items()):
        print(f"   {cat:30s}: {score*100:5.1f}%")
    
    print(f"\nâ­ By Difficulty:")
    for diff, score in sorted(summary['by_difficulty'].items()):
        print(f"   {diff:10s}: {score*100:5.1f}%")
    
    if summary['errors']:
        print(f"\nâŒ Errors ({len(summary['errors'])}):")
        for err in summary['errors'][:5]:
            print(f"   - {err['qa_id']}")
    
    if summary['low_scores']:
        print(f"\nâš ï¸  Low Scores (<50%, {len(summary['low_scores'])}):")
        for low in summary['low_scores'][:5]:
            print(f"   - {low['qa_id']}: {low['score']:.2f}")
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    if save_report:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = model_dir / f"evaluation_{timestamp}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ Detailed report saved: {report_file.name}")
    
    return summary

# ============================================
# å¯¹æ¯”å¤šä¸ªæ¨¡å‹
# ============================================

def compare_models(model_names: List[str], evaluator: SpreadsheetEvaluator):
    """å¯¹æ¯”å¤šä¸ªæ¨¡å‹çš„æ€§èƒ½"""
    
    results = {}
    for model_name in model_names:
        summary = evaluate_model(model_name, evaluator, save_report=False)
        if summary:
            results[model_name] = summary
    
    if not results:
        print("âŒ No results to compare")
        return
    
    # ç”Ÿæˆå¯¹æ¯”è¡¨
    print(f"\n{'='*70}")
    print(f"ğŸ“Š Model Comparison")
    print(f"{'='*70}")
    
    print(f"\n{'Model':<30s} {'Overall':>10s} {'Easy':>8s} {'Medium':>8s} {'Hard':>8s}")
    print("-" * 70)
    
    for model_name, summary in results.items():
        overall = summary['overall']['percentage']
        by_diff = summary['by_difficulty']
        easy = by_diff.get('easy', 0) * 100
        medium = by_diff.get('medium', 0) * 100
        hard = by_diff.get('hard', 0) * 100
        
        print(f"{model_name:<30s} {overall:>9.1f}% {easy:>7.1f}% {medium:>7.1f}% {hard:>7.1f}%")
    
    # ä¿å­˜å¯¹æ¯”æŠ¥å‘Š
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    comparison_file = Path("outputs") / f"comparison_{timestamp}.json"
    with open(comparison_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ Comparison saved: {comparison_file}")

# ============================================
# ä¸»ç¨‹åº
# ============================================

if __name__ == "__main__":
    evaluator = SpreadsheetEvaluator("qa.json")
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "compare":
            # å¯¹æ¯”æ‰€æœ‰æ¨¡å‹
            outputs_dir = Path("outputs")
            model_names = [d.name for d in outputs_dir.iterdir() if d.is_dir()]
            compare_models(model_names, evaluator)
        else:
            # è¯„ä¼°å•ä¸ªæ¨¡å‹
            evaluate_model(sys.argv[1], evaluator)
    else:
        # è¯„ä¼°æ‰€æœ‰æ¨¡å‹
        outputs_dir = Path("outputs")
        if not outputs_dir.exists():
            print("âŒ No outputs directory found")
            sys.exit(1)
        
        for model_dir in outputs_dir.iterdir():
            if model_dir.is_dir():
                evaluate_model(model_dir.name, evaluator)