import json
import base64
import time
from pathlib import Path
from typing import Dict, List
import google.generativeai as genai
from datetime import datetime
import os
from dotenv import load_dotenv

# ============================================
# åŠ è½½ç¯å¢ƒå˜é‡
# ============================================

# ä».envæ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è·å–ç¯å¢ƒå˜é‡
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
if not GEMINI_API_KEY:
    raise ValueError("âŒ GEMINI_API_KEY not found in .env file!")

# è®¾ç½®ä»£ç†ï¼ˆå¦‚æœåœ¨.envä¸­æœ‰é…ç½®ï¼‰
HTTP_PROXY = os.getenv('HTTP_PROXY')
HTTPS_PROXY = os.getenv('HTTPS_PROXY')

if HTTP_PROXY:
    os.environ['HTTP_PROXY'] = HTTP_PROXY
if HTTPS_PROXY:
    os.environ['HTTPS_PROXY'] = HTTPS_PROXY

# é…ç½®Gemini API
genai.configure(api_key=GEMINI_API_KEY)

# ============================================
# é…ç½®éƒ¨åˆ†
# ============================================

QA_FILE = "qa.json"

# æ¨¡å‹é…ç½®
MODEL_NAME = os.getenv('MODEL_NAME', 'gemini-3-flash-preview')

# ç‰ˆæœ¬é…ç½® - å…³é”®ï¼
# å¯é€‰å€¼: "vanilla" (æ— æ ¼å¼) æˆ– "formatted" (æœ‰æ ¼å¼)
VERSION = os.getenv('VERSION', 'vanilla')  # ä».envè¯»å–ï¼Œé»˜è®¤vanilla

# æ ¹æ®ç‰ˆæœ¬é€‰æ‹©å›¾ç‰‡æ–‡ä»¶
IMAGE_FILES = {
    "vanilla": {
        "test_temp": "Test_vanilla.png",
        "test2_temp": "Test2_vanilla.png"
    },
    "formatted": {
        "test_temp": "Test_formatted.png",
        "test2_temp": "Test2_formatted.png"
    }
}

# ç”Ÿæˆé…ç½®
GENERATION_CONFIG = {
    "temperature": float(os.getenv('TEMPERATURE', '0.7')),
    "top_p": float(os.getenv('TOP_P', '0.95')),
    "max_output_tokens": int(os.getenv('MAX_OUTPUT_TOKENS', '4096')),
}

# ============================================
# æ–‡ä»¶ç®¡ç†
# ============================================

def get_model_directory(model_name: str, version: str) -> Path:
    """
    æ ¹æ®æ¨¡å‹åç§°å’Œç‰ˆæœ¬åˆ›å»ºå¹¶è¿”å›è¾“å‡ºç›®å½•
    ä¾‹å¦‚: gemini-3-pro-preview/vanilla/
    """
    # åˆ›å»ºoutputsæ ¹ç›®å½•
    base_dir = Path("outputs")
    base_dir.mkdir(exist_ok=True)
    
    # åˆ›å»ºæ¨¡å‹ä¸“å±ç›®å½•
    model_dir = base_dir / model_name
    model_dir.mkdir(exist_ok=True)
    
    # åˆ›å»ºç‰ˆæœ¬å­ç›®å½•
    version_dir = model_dir / version
    version_dir.mkdir(exist_ok=True)
    
    return version_dir

def get_progress_file(model_name: str, version: str) -> Path:
    """è·å–è¿›åº¦æ–‡ä»¶è·¯å¾„"""
    return get_model_directory(model_name, version) / "progress.json"

def get_responses_file(model_name: str, version: str) -> Path:
    """è·å–æœ€ç»ˆå“åº”æ–‡ä»¶è·¯å¾„ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    return get_model_directory(model_name, version) / f"responses_{timestamp}.json"

def get_evaluation_file(model_name: str, version: str) -> Path:
    """è·å–è¯„ä¼°æŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    return get_model_directory(model_name, version) / f"evaluation_{timestamp}.json"

# ============================================
# å·¥å…·å‡½æ•°
# ============================================

def load_progress(model_name: str, version: str) -> Dict:
    """åŠ è½½å·²å®Œæˆçš„è¿›åº¦"""
    progress_file = get_progress_file(model_name, version)
    if progress_file.exists():
        with open(progress_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

def save_progress(responses: Dict, model_name: str, version: str):
    """ä¿å­˜è¿›åº¦"""
    progress_file = get_progress_file(model_name, version)
    with open(progress_file, 'w', encoding='utf-8') as f:
        json.dump(responses, f, indent=2, ensure_ascii=False)

def load_image(image_path: str) -> bytes:
    """åŠ è½½å›¾ç‰‡"""
    with open(image_path, 'rb') as f:
        return f.read()

def load_qa_data(qa_file: str) -> Dict:
    """åŠ è½½QAæ•°æ®"""
    with open(qa_file, 'r', encoding='utf-8') as f:
        return json.load(f)

def enhance_prompt_for_range(question: str, answer_type: str) -> str:
    """ä¸ºrangeç±»å‹å¢å¼ºprompt"""
    if answer_type in ['range', 'cell_range']:
        return f"""{question}

**Important**: Please provide your answer in the format 'Cell1:Cell2' (e.g., A1:F10).
- Use the top-left cell and bottom-right cell notation
- Format: ColumnRow:ColumnRow (e.g., B2:E14)
- Be concise and direct"""
    return question

def call_gemini(image_path: str, question: str, answer_type: str, model_name: str) -> str:
    """
    è°ƒç”¨Gemini API
    """
    try:
        # åŠ è½½å›¾ç‰‡
        image_data = load_image(image_path)
        
        # å¢å¼ºprompt
        enhanced_question = enhance_prompt_for_range(question, answer_type)
        
        # åˆ›å»ºæ¨¡å‹
        model = genai.GenerativeModel(
            model_name=model_name,
            generation_config=GENERATION_CONFIG
        )
        
        # è°ƒç”¨API
        response = model.generate_content([
            enhanced_question,
            {"mime_type": "image/png", "data": image_data}
        ])
        
        return response.text
        
    except Exception as e:
        error_msg = str(e)
        print(f"       âŒ Error: {error_msg[:100]}")
        return f"ERROR: {error_msg}"

# ============================================
# ä¸»ç¨‹åº
# ============================================

def main():
    print("=" * 70)
    print(f"Gemini API - Spreadsheet Understanding Evaluation")
    print(f"Model: {MODEL_NAME}")
    print(f"Version: {VERSION} ({'æ— æ ¼å¼' if VERSION == 'vanilla' else 'æœ‰æ ¼å¼'})")
    print(f"Output Directory: ./outputs/{MODEL_NAME}/{VERSION}/")
    print("=" * 70)
    print()
    
    # éªŒè¯API key
    if not GEMINI_API_KEY or GEMINI_API_KEY == "YOUR_API_KEY_HERE":
        print("âŒ Error: Please set GEMINI_API_KEY in .env file")
        return
    
    print(f"âœ… API Key loaded: {GEMINI_API_KEY[:10]}...{GEMINI_API_KEY[-4:]}")
    if HTTP_PROXY:
        print(f"âœ… Proxy configured: {HTTP_PROXY}")
    print()
    
    # éªŒè¯ç‰ˆæœ¬é…ç½®
    if VERSION not in IMAGE_FILES:
        print(f"âŒ Error: Invalid VERSION '{VERSION}'. Must be 'vanilla' or 'formatted'")
        return
    
    # è·å–å½“å‰ç‰ˆæœ¬çš„å›¾ç‰‡é…ç½®
    current_images = IMAGE_FILES[VERSION]
    
    # éªŒè¯å›¾ç‰‡æ–‡ä»¶å­˜åœ¨
    print("ğŸ“· Checking image files...")
    missing_files = []
    for file_id, image_path in current_images.items():
        if not Path(image_path).exists():
            missing_files.append(image_path)
            print(f"   âš ï¸  Missing: {image_path}")
        else:
            print(f"   âœ“ Found: {image_path}")
    
    if missing_files:
        print(f"\nâŒ Error: {len(missing_files)} image file(s) not found!")
        return
    print()
    
    # åˆ›å»ºæ¨¡å‹è¾“å‡ºç›®å½•
    model_dir = get_model_directory(MODEL_NAME, VERSION)
    print(f"ğŸ“ Output directory: {model_dir}")
    print()
    
    # åŠ è½½QAæ•°æ®
    print("ğŸ“‚ Loading QA data...")
    qa_data = load_qa_data(QA_FILE)
    total_questions = len(qa_data['qa_pairs'])
    print(f"   Loaded {total_questions} questions")
    
    # åŠ è½½è¿›åº¦
    print("ğŸ“¥ Loading progress...")
    vlm_responses = load_progress(MODEL_NAME, VERSION)
    completed = len(vlm_responses)
    print(f"   Already completed: {completed}/{total_questions}")
    print()
    
    if completed == total_questions:
        print("âœ… All questions already completed!")
        print(f"ğŸ“„ Results saved in: {model_dir}")
        return
    
    # æ”¶é›†å›ç­”
    print(f"ğŸ¤– Calling {MODEL_NAME} API...")
    start_time = time.time()
    
    for i, qa in enumerate(qa_data['qa_pairs'], 1):
        qa_id = qa['id']
        
        # è·³è¿‡å·²å®Œæˆçš„
        if qa_id in vlm_responses:
            print(f"   [{i}/{total_questions}] {qa_id} - âœ… Already done")
            continue
        
        file_id = qa['file_id']
        question = qa['question']
        answer_type = qa.get('answer_type', 'single_value')
        
        # è·å–å½“å‰ç‰ˆæœ¬çš„å›¾ç‰‡
        image_path = current_images.get(file_id)
        if not image_path:
            print(f"   âš ï¸  Warning: No image found for {file_id}")
            continue
        
        print(f"   [{i}/{total_questions}] {qa_id}...")
        print(f"       Image: {image_path}")
        print(f"       Q: {question[:50]}...")
        
        # è°ƒç”¨API
        response = call_gemini(image_path, question, answer_type, MODEL_NAME)
        vlm_responses[qa_id] = response
        
        # æ˜¾ç¤ºå›ç­”ï¼ˆæˆªæ–­æ˜¾ç¤ºï¼‰
        display_answer = response[:80] + "..." if len(response) > 80 else response
        print(f"       A: {display_answer}")
        
        # å®æ—¶ä¿å­˜è¿›åº¦
        save_progress(vlm_responses, MODEL_NAME, VERSION)
        print(f"       ğŸ’¾ Progress saved ({len(vlm_responses)}/{total_questions})")
        print()
    
    # è®¡ç®—è€—æ—¶
    elapsed_time = time.time() - start_time
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    final_file = get_responses_file(MODEL_NAME, VERSION)
    with open(final_file, 'w', encoding='utf-8') as f:
        json.dump(vlm_responses, f, indent=2, ensure_ascii=False)
    
    print("=" * 70)
    print("âœ… All responses collected!")
    print(f"â±ï¸  Time elapsed: {elapsed_time:.1f} seconds")
    print(f"ğŸ“Š Total questions: {total_questions}")
    print(f"ğŸ“ Results saved to: {model_dir}")
    print(f"   - Progress: {get_progress_file(MODEL_NAME, VERSION).name}")
    print(f"   - Responses: {final_file.name}")
    print("=" * 70)

if __name__ == "__main__":
    main()